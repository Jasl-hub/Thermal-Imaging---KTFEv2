# -*- coding: utf-8 -*-
"""ThermoEmo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FreVTBfuFu8t48p3OErGVq-sADq-mZgx
"""

import numpy as np
import pandas as pd
import os
import cv2
import time
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from pylab import rcParams
rcParams['figure.figsize'] = 20, 10

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn import metrics #for confusion matrix

from keras.utils import to_categorical
import keras
from keras import utils
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D
from keras.metrics import categorical_accuracy
from keras.models import model_from_json
from keras.callbacks import ModelCheckpoint
from keras.optimizers import *
from keras.layers import BatchNormalization
import tensorflow as tf

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/thermal.zip -d '/content/EM'

print(os.listdir("/content/EM/thermal"))

# Specify the directory path containing the class folders
directory_path = '/content/EM/thermal'

# List the contents of the directory
contents = os.listdir(directory_path)

# Filter out the undesired directory (.ipynb_checkpoints)
class_folders = [item for item in contents if os.path.isdir(os.path.join(directory_path, item)) and item != '.ipynb_checkpoints']

# Print the list of class folders
print(class_folders)

data_path = "/content/EM/thermal"
data_dir_list = os.listdir(data_path)
img_data_list=[]
for dataset in class_folders:
    print(dataset)
    img_list=os.listdir(data_path+'/'+ dataset)
    print ('Loaded the images of dataset-'+'{}\n'.format(dataset))
    for img in img_list:
     # print(img)
      if img.endswith(".jpg"):
        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )
        # input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)
        input_img_resize=cv2.resize(input_img,(200, 200))
        img_data_list.append(input_img_resize)

def remove_unwanted_files(folder_path, keep_extensions=[".jpg"]):
    # Normalize extensions to lowercase
    keep_extensions = [ext.lower() for ext in keep_extensions]

    # Iterate over files in the folder
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        # Check if the item is a file and if its extension is not in the keep list
        if os.path.isfile(file_path) and not any(filename.lower().endswith(ext) for ext in keep_extensions):
            os.remove(file_path)
            print(f"Removed: {file_path}")

# Replace 'path/to/your/folder' with the actual path to your folder
folder_path = '/content/EM/thermal/neutral'
remove_unwanted_files(folder_path)

img_data = np.array(img_data_list)
img_data = img_data.astype('float32')
img_data = img_data/255
img_data.shape

num_classes = 7
num_of_samples = img_data.shape[0]
labels = np.ones((num_of_samples,),dtype='int64')

labels[0:384]=0   #anger(385)
labels[385:782]=1 #Disgust (399-1)
labels[783:1138]=2  #Surprise (357-1)
labels[1139:1501]=3  #neutral(364-1)
labels[1502:1824]=4 #Fear(323)
labels[1825:2193]=5 #Sad (369)
labels[2194:2593]=6 #Happy (401-1)
names = ['anger', 'disgust', 'surprise', 'neutral', 'fear', 'sad', 'happy']


def getLabel(id):
    return ['anger', 'disgust', 'surprise', 'neutral', 'fear', 'sad', 'happy'][id]

"""train_test_split

"""

Y = to_categorical(labels, num_classes=num_classes)
#Shuffle the dataset
#x,y = shuffle(img_data,Y, random_state=2)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(img_data, Y, test_size=0.15, random_state=2)
#sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)

x_test=X_test

input_shape=(200, 200,3)

model = Sequential()
model.add(Conv2D(12, (5, 5), input_shape=input_shape, padding='same', activation = 'relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Conv2D(20, (5, 5), padding='valid', activation = 'relu'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3), activation = 'relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(256, activation = 'relu'))
##model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(7, activation = 'softmax'))

##adam = keras.optimizers.Adam(lr=0.001)
model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')
model.summary()
model.get_config()
model.layers[0].get_config()
model.layers[0].input_shape
model.layers[0].output_shape
model.layers[0].get_weights()
np.shape(model.layers[0].get_weights()[0])
model.layers[0].trainable

from keras import callbacks
filename='model_train_new.csv'
filepath="Best-weights-my_model-{epoch:03d}-{loss:.4f}-{acc:.4f}.hdf5"

csv_log=callbacks.CSVLogger(filename, separator=',', append=False)
checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [csv_log,checkpoint]
callbacks_list = [csv_log]

import time
from keras.callbacks import Callback

class TrainingTimeCallback(Callback):
    def on_train_begin(self, logs={}):
        self.start_time = time.time()

    def on_train_end(self, logs={}):
        end_time = time.time()
        training_time = end_time - self.start_time
        print("Training time:", training_time, "seconds")

# Create an instance of the TrainingTimeCallback
training_time_callback = TrainingTimeCallback()

# Fit the model with the training time callback
hist = model.fit(X_train, y_train, batch_size=7, epochs=50, verbose=1,
                 validation_data=(X_test, y_test), callbacks=[callbacks_list, training_time_callback])

"""**validation**"""

hist = model.fit(X_train, y_train, batch_size=7, epochs=100, verbose=1, validation_data=(X_test, y_test),callbacks=callbacks_list)

score = model.evaluate(X_test, y_test, verbose=0)
print('Test Loss:', score[0])
print('Test accuracy:', score[1])

test_image = X_test[1:2]
print (test_image.shape)

import time
from keras.callbacks import Callback

class TestingTimeCallback(Callback):
    def on_test_begin(self, logs={}):
        self.start_time = time.time()

    def on_test_end(self, logs={}):
        end_time = time.time()
        testing_time = end_time - self.start_time
        print("Testing time:", testing_time, "seconds")

# Create an instance of the TestingTimeCallback
testing_time_callback = TestingTimeCallback()

# Evaluate the model with the testing time callback
score = model.evaluate(X_test, y_test, verbose=0, callbacks=[testing_time_callback])
print('Test Loss:', score[0])
print('Test accuracy:', score[1])

print(model.predict(test_image))
np.argmax(model.predict(test_image), axis=-1)
print(y_test[0:1])
res = np.argmax(model.predict(X_test[0:9]), axis=-1)
plt.figure(figsize=(10, 10))


for i in range(0,9):
   plt.subplot(330 + 1 + i)
   plt.imshow(x_test[i])
   plt.gca().get_xaxis().set_ticks([])
   plt.gca().get_yaxis().set_ticks([])
   plt.ylabel('prediction = %s' % getLabel(res[i]), fontsize=16)
# show the plot
plt.show()

# visualizing losses and accuracy
get_ipython().run_line_magic('matplotlib', 'inline')

train_loss=hist.history['loss']
val_loss=hist.history['val_loss']
train_acc=hist.history['accuracy']
val_acc=hist.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='Train Loss')
plt.plot(epochs,val_loss,'b', label='Val Loss')
plt.title('Train Loss vs Val Loss')
plt.legend()
plt.figure()
plt.plot(epochs,train_acc,'r', label='Train Accuracy')
plt.plot(epochs,val_acc,'b', label='Val Accuracy')
plt.title('Train Accuracy vs Val Accuracy')
plt.legend()
plt.figure()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score

# Assuming model, X_test, and y_test are already defined
# Make predictions on the test data
y_pred = model.predict(X_test)

# Convert predictions and true labels to class labels
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = np.argmax(y_test, axis=1)

# Compute the confusion matrix
cm = confusion_matrix(y_test_classes, y_pred_classes)

# Normalize the confusion matrix by row (i.e., by the number of samples in each class)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

# Plot the normalized confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=range(num_classes))
disp.plot(cmap=plt.cm.Blues, values_format='.2%')  # Use percentage format

plt.title('Normalized Confusion Matrix for 7 Classes')
plt.show()

# Calculate accuracy
accuracy = accuracy_score(y_test_classes, y_pred_classes)
print(f'Accuracy: {accuracy:.2f}')

# Calculate precision, recall, and f1-score
report = classification_report(y_test_classes, y_pred_classes, target_names=[f'Class {i}' for i in range(num_classes)])
print('Classification Report:')
print(report)

# Display images with predicted and true labels
def plot_images(images, true_labels, pred_labels, num_images=10):
    plt.figure(figsize=(20,20))
    for i in range(num_images):
        plt.subplot(1, num_images, i+1)
        plt.imshow(images[i], cmap='gray')  # Adjust shape if needed for your data
        plt.title(f"True: {true_labels[i]}\nPred: {pred_labels[i]}")
        plt.axis('off')
    plt.show()

# Display the first 10 images from the test set with true and predicted labels
plot_images(X_test, y_test_classes, y_pred_classes, num_images=10)